{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["In C:\\Users\\Desktop\\Anaconda3\\envs\\ngym36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n","The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n","In C:\\Users\\Desktop\\Anaconda3\\envs\\ngym36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n","The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n","In C:\\Users\\Desktop\\Anaconda3\\envs\\ngym36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n","In C:\\Users\\Desktop\\Anaconda3\\envs\\ngym36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n","The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n","In C:\\Users\\Desktop\\Anaconda3\\envs\\ngym36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n","The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n","In C:\\Users\\Desktop\\Anaconda3\\envs\\ngym36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n","The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n","In C:\\Users\\Desktop\\Anaconda3\\envs\\ngym36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n","The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n","In C:\\Users\\Desktop\\Anaconda3\\envs\\ngym36\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n","The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n","Using TensorFlow backend.\n"]}],"source":["import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","\n","from feature_engine.imputation import MeanMedianImputer, ArbitraryNumberImputer\n","from feature_engine.outliers import Winsorizer\n","from feature_engine.wrappers import SklearnTransformerWrapper\n","from feature_engine.discretisation import EqualFrequencyDiscretiser\n","from feature_engine.discretisation.arbitrary import ArbitraryDiscretiser\n","from feature_engine.selection import DropFeatures\n","from feature_engine.selection import SmartCorrelatedSelection\n","from feature_engine.selection import RecursiveFeatureAddition\n","from feature_engine.encoding import MeanEncoder\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_validate\n","from sklearn.preprocessing import RobustScaler , MinMaxScaler\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import Pipeline \n","from sklearn.feature_selection import SequentialFeatureSelector\n","\n","s_path = os.path.dirname(os.path.realpath(__file__))\n","sys.path.append(s_path)\n","\n","pd.set_option('display.max_columns',100)\n","pd.set_option('precision', 3)\n","pd.set_option('display.float_format', lambda x: '%.3f' % x)\n","\n","import tm_teoriaMvto_base_prep as base_prep\n","import tm_teoriaMvto_label as tm_label\n","import tm_teoriaMvto_train as tm_train\n","import tm_teoriaMvto_ft_eng as ft_eng\n","# import tm_teoriaMvto_ft_sel as ft_sel\n","\n","# models to test\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from xgboost import XGBClassifier\n","\n","# optimizing ensemble\n","from skopt.space import Integer\n","from skopt.space import Real\n","from skopt.space import Categorical\n","from skopt.utils import use_named_args\n","from skopt import forest_minimize\n","from skopt.callbacks import DeltaYStopper\n","\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["path_files = os.path.join(s_path, 'historical_files')\n","\n","# base prep control\n","EXPORT_X = False\n","EXPORT_Y = False\n","\n","# params labeling\n","s_prefix = 'ft_'\n","s_lbl_type = 'c_binary'\n","f_th = 0.2\n","b_dist = True\n","b_percent = True\n","b_custom = True\n","\n","# params loading\n","s_regime = 'mi_up'\n","\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["if EXPORT_X:\n","  df_data = base_prep.import_sampling(path_files= os.path.join(path_files, 'FeaturesFiles'), prefix_files= s_prefix)\n","\n","  # create new features before extracting samples - using full data needed by lag features\n","  df_data = ft_eng.BasicFeatures().transform(X=df_data)\n","\n","  df_X = base_prep.ft_export(df_data, path_files= os.path.join(path_files, 'TrainFiles'), prefix= s_prefix)\n","  y_sc = base_prep.y_export(df_data, path_files= os.path.join(path_files, 'TrainFiles'), prefix= s_prefix)\n","\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["y outfile ready for configuration: prefix_ft__type_c_binary_th_0.2_dist_True_percent_True_custom_True\n"]}],"source":["y_config = 'prefix_'+ s_prefix +'_type_'+ s_lbl_type +'_th_'+ str(f_th) +'_dist_'+ str(b_dist) +'_percent_'+str(b_percent)+'_custom_'+str(b_custom)\n","\n","if EXPORT_Y:\n","  df_label = None \n","  if EXPORT_X: df_label = y_sc\n","\n","  # specify df_data = None (default) to load pickle from s_path + 'y_prep_data.pkl'\n","  label = tm_label.Labeling(df_data = df_label, label_type= s_lbl_type, b_dist_to_high= b_dist, \n","                            s_path = os.path.join(path_files, 'TrainFiles'), prefix_files = s_prefix, th_label_y1=f_th, \n","                            b_percent_freq=b_percent, b_custom_dir=b_custom)\n","\n","  df_y = label.apply_label(s_model_return = s_regime)  # it will export 4 pickle files to label_obj.s_path\n","  # y_config = label.s_name\n","\n","y_outfile = os.path.join(os.path.join(path_files, 'TrainFiles'), 'y_' + y_config +'.pkl')\n","\n","if os.path.exists(y_outfile):\n","  print('y outfile ready for configuration: {}'.format(y_config))\n","else:\n","  print('WARNING: y outfile not found for configuration. Define EXPORT_Y = True and try again {}'.format(y_config))\n","\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["l_prop_08 = ['escora_bid_2.5_0.8', 'escora_ask_2.5_0.8', 'escora_bid_3.5_0.8', \n","            'escora_ask_3.5_0.8', 'escora_bid_4.5_0.8', 'escora_ask_4.5_0.8', ]\n","\n","l_prop_12 = ['escora_bid_2.5_1.2', 'escora_ask_2.5_1.2', 'escora_bid_3.5_1.2',\n","            'escora_ask_3.5_1.2', 'escora_ask_4.5_1.2', 'escora_bid_4.5_1.2', ]\n","\n","l_prop_2 = [ 'escora_bid_2.5_2', 'escora_ask_2.5_2', 'escora_bid_3.5_2',\n","            'escora_ask_3.5_2', 'escora_bid_4.5_2', 'escora_ask_4.5_2', ]\n","\n","l_col_log = ['agg_net_d', 'aggbig_net_d', 'vol_trd', 'vol_big', 'big_v', 'vol_trd_aux', \n","   'vol_big_aux', 'big_v_aux', 'loc_agg_net_d', 'big_c', 'big_c_aux',\n","   'loc_aggbig_net_d', 'agg_net_m', 'agg_net_m_aux', 'abagg', 'abagg_aux',\n","   'aggbig_net_m', 'aggbig_net_m_aux', 'loc_agg_net_m', 'loc_aggbig_net_m',\n","   'loc_agg_net_m_aux', 'loc_aggbig_net_m_aux', 'loc_aggbig_c_m', 'loc_aggbig_v_m', \n","   'loc_aggbig_c_m_aux', 'loc_aggbig_v_m_aux', 'abs_v', 'abs_c', 'aggpior_v', 'aggpior_v_aux', \n","   'aggpior_c', 'aggpior_c_aux', 'agg_net_10', 'agg_net_40', 'agg_net_80', 'loc_agg_net_10',\n","   'aggbig_net_10', 'aggpior_DIF', 'aggpior_DIF_30', 'abs_DIF', 'abs_DIF_30',\n","   'abagg_10', 'aggpior_aux_DIF', ]\n","\n","l_side_drop = ['big_c','big_v','aggpior_c','aggpior_v','loc_aggbig_c_m','loc_aggbig_v_m','pagg_c_best',\n","  'pagg_c_best_0.5','pagg_c_best_0.7','pagg_c_best_0.9','pagg_v_best','pagg_v_best_0.5','pagg_v_best_0.7',\n","  'pagg_v_best_0.9','abs_c','abs_v','int_c','int_c_0.6','int_c_0.7','int_c_0.8','int_c_0.9','int_dif_c',\n","  'int_v','int_v_0.6','int_v_0.7','int_v_0.8','int_v_0.9','int_dif_v','imp_c','imp_c_0.6','imp_c_0.7',\n","  'imp_c_0.8','imp_c_0.9','imp_v','imp_v_0.6','imp_v_0.7','imp_v_0.8','imp_v_0.9','escora_bid_2.5_1.2',\n","  'escora_bid_2.5_2','escora_ask_2.5_1.2','escora_ask_2.5_2','escora_bid_3.5_1.2','escora_bid_3.5_2',\n","  'escora_ask_3.5_1.2','escora_ask_3.5_2','escora_bid_4.5_0.8','escora_bid_4.5_2','escora_ask_4.5_0.8',\n","  'escora_ask_4.5_2','movesc_bid_2.5','movesc_ask_2.5','movesc_bid_2.5_0.5','movesc_ask_2.5_0.5',\n","  'movesc_bid_2.5_0.7','movesc_ask_2.5_0.7','movesc_bid_3.5','movesc_ask_3.5','movesc_bid_3.5_0.7',\n","  'movesc_ask_3.5_0.7','movesc_bid_3.5_0.9','movesc_ask_3.5_0.9','movesc_bid_4.5','movesc_ask_4.5',\n","  'movesc_bid_4.5_0.5','movesc_ask_4.5_0.5','movesc_bid_4.5_0.7','movesc_ask_4.5_0.7','depth_bid7','depth_ask7'\n","  ]\n","\n","l_side_drop_aux = ['big_c_aux','big_v_aux','aggpior_c_aux','aggpior_v_aux','loc_aggbig_c_m_aux',\n","  'loc_aggbig_v_m_aux','pagg_c_best_aux','pagg_c_best_0.5_aux','pagg_c_best_0.7_aux',\n","  'pagg_c_best_0.9_aux','pagg_v_best_aux','pagg_v_best_0.5_aux', 'pagg_v_best_0.7_aux',\n","  'pagg_v_best_0.9_aux'\n","  ]\n","\n","l_ft_aux = [\n","  'vol_trd_aux', 'n_trd_aux','vol_big_aux','n_big_aux','vol_big_ratio_aux','big_c_aux','big_v_aux',\n","  'aggpior_c_aux','aggpior_v_aux','aggimb_aux','aggimb_big_aux','n_aggimb_aux','agg_net_m_aux',\n","  'aggbig_net_m_aux','loc_aggbig_c_m_aux','loc_aggbig_v_m_aux','loc_agg_net_m_aux','loc_aggbig_net_m_aux',\n","  'loc_agg_imb_m_aux','loc_aggbig_imb_m_aux','pagg_c_best_aux','pagg_c_best_0.5_aux','pagg_c_best_0.7_aux',\n","  'pagg_c_best_0.9_aux','pagg_v_best_aux','pagg_v_best_0.5_aux','pagg_v_best_0.7_aux','pagg_v_best_0.9_aux',\n","  'abagg_aux','n_p_aux','aggpior_aux_DIF','pagg_aux_DIF'\n","  ]\n","\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["load_split: X, y data load and split complete!\n"]}],"source":["test_size = 0.2\n","\n","if not EXPORT_X:\n","  df_X = tm_train.load_models('X_samples_'+s_prefix, os.path.join(path_files, 'TrainFiles'))  # search for path_files/s_regime.pkl\n","\n","X = df_X.loc[df_X['model'] == s_regime].drop(columns='model')\n","\n","if not EXPORT_Y:\n","  df_y = tm_train.load_models('y_' + y_config, os.path.join(path_files, 'TrainFiles'))\n","  y = df_y.loc[df_y['model'] == s_regime].drop(columns='model')\n","else:\n","  y = df_y.drop(columns='model')\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, shuffle = False)\n","\n","print('load_split: X, y data load and split complete!')\n","\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# DEBUG: prop_features contaning np.NaN = (pd.isna(X_train).sum()>0).sort_values(ascending=False).head(50)\n","X_train = ft_eng.PropImputer(0.8, l_prop_08).transform(X_train)\n","X_train = ft_eng.PropImputer(1.2, l_prop_12).transform(X_train)\n","X_train = ft_eng.PropImputer(2, l_prop_2).transform(X_train)\n","\n","median_inputer = MeanMedianImputer(variables=['PA_down',])\n","X_train = median_inputer.fit_transform(X_train)\n","\n","nan_imputer = ArbitraryNumberImputer(0.0, variables=['ohlc_10','ohlc_50'])\n","X_train = nan_imputer.fit_transform(X_train)\n","\n","X_train = ft_eng.DifAll().transform(X_train)\n","X_train = ft_eng.LogVolume(l_col_log).transform(X_train)\n","\n","if s_regime[:2] == 'mw':\n","  # for now, removing all side columns\n","  l_cols_drop = l_side_drop + l_ft_aux + ['smart_price', 'sspread']\n","else:\n","  l_cols_drop = l_side_drop + l_side_drop_aux + ['s_run', 'n_p_aux', 'smart_price', 'sspread']\n","\n","# TODO: INCREMENTAR L COLS DROP TAMBEM QUANDO LABEL NAO FOR DIST-TO-HIGH, RETIRANDO AS COLUNAS DE VARIACAO DE PRECO!\n","\n","X_train.drop(columns=l_cols_drop, inplace=True)\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["X_test = ft_eng.PropImputer(0.8, l_prop_08).transform(X_test)\n","X_test = ft_eng.PropImputer(1.2, l_prop_12).transform(X_test)\n","X_test = ft_eng.PropImputer(2, l_prop_2).transform(X_test)\n","\n","X_test = median_inputer.transform(X_test)\n","\n","X_test = nan_imputer.transform(X_test)\n","\n","X_test = ft_eng.DifAll().transform(X_test)\n","X_test = ft_eng.LogVolume(l_col_log).transform(X_test)\n","\n","X_test.drop(columns=l_cols_drop, inplace=True)\n","\n",""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","# TODO: move function to ft_selection.py\n","def list_sub(lst1, lst2):\n","  final_list = list(set(lst1) - set(lst2))\n","  return final_list\n","\n","\n","def list_union(*l_lists):\n","  l_union = l_lists[0]\n","  if len(l_lists) > 1:\n","    for i in range(len(l_lists)-1):\n","      l_union = set(l_union) | set(l_lists[i+1])\n","  return list(l_union)\n","\n","l_duplicate = ['loc_agg_net_m',]  # _2 ft comes from ft_eng.duplicate()\n","\n","l_cap_1 = ['vewma_c_v', 'vewmag_dif', 'book_imb', 'book_imb_dif', ]\n","\n","l_cap_5 = ['PA_up', 'PA_down', ]\n","\n","l_cap_10 = ['n_big_aux', ]\n","\n","l_cap_default = list_sub(X_train.columns.to_list(), list_union(l_cap_1, l_cap_5, l_cap_10))\n","\n","l_bins_q2 = ['loc_agg_net_m_2',]  # _2 ft comes from ft_eng.duplicate()\n","\n","l_bins_q4 = ['ohlc_10','ohlc_50',]\n","\n","l_bins_q5 = ['loc_aggbig_net_m','vol_trd_aux','aggbig_net_m_aux','book_imb','rng_ewma_dif',\n","            'rng_ewma_dif_40','rng_ewma_dif_80','vewma_10','vewma_g_p_10','aggbig_net_10',\n","]\n","\n","l_bins_q6 = ['agg_net_d', 'aggbig_net_d', 'loc_agg_imb_m_aux',]\n","\n","l_bins_q8 = ['rng_ewma','vewma_g_p','vewmag_dif','n_trd_aux','abagg_aux','escora_bid_2.5_0.8','escora_ask_2.5_0.8',\n","            'escora_bid_3.5_0.8','escora_ask_3.5_0.8','escora_bid_4.5_1.2','escora_ask_4.5_1.2','movesc_bid_2.5_0.9',\n","            'movesc_ask_2.5_0.9','movesc_bid_3.5_0.5','movesc_ask_3.5_0.5','movesc_bid_4.5_0.9','movesc_ask_4.5_0.9',\n","            'msg_imb','rng_smart_10','imp_DIF_10','imp_DIF_50','agg_net_80','imp_FCAST_40','aggpior_DIF',\n","            'book_imb_mean_dif_cp','msg_imb_mean_10','escora_4.5_0.8_DIF',\n","]\n","\n","l_bins_q10 = ['vol_big_ratio','aggimb','n_aggimb','agg_net_m','loc_aggbig_imb_m','abagg','n_p','vewma',\n","            'vewma_c_v', 'aggimb_aux','aggimb_big_aux','agg_net_m_aux','loc_aggbig_net_m_aux','smart_price_dif',\n","            'smart_price_50','rng_smart_50','agg_net_10','agg_net_40','loc_agg_net_10','int_DIF_10','abagg_10',\n","            'book_imb_mean_10','book_imb_mean_dif_lp','msg_imb_mean_40','msg_imb_mean_dif_lp','msg_imb_mean_dif_cp',\n","            'sspread_mean','movesc_2.5_0.7_DIF','msg_imb_mean_40_ABS','loc_agg_net_m',\n","] \n","\n","d_bins_arbitrary = {\n","            'n_trd':  [-0.01, 70, 180, 280, 380, 5000000],\n","            'vol_trd':  [0, 7.237, 7.55, 10000],\n","            'aggimb_big':\t\t[-1.1, -0.4, 0.4, 1.1],\n","            'aggbig_net_m':\t\t[-10000, -5, +10000],\n","            'chgfreq':\t\t[-0.01, 0.167, 0.280, 1.01],\n","            'last_d_s':\t\t[-1.01, -0.5, 0.5, 1.01],\n","            'loc_agg_net_d':\t\t[-10000, -6.4, 5.2, 5.7, 10000],\n","            'loc_aggbig_net_d':\t\t[-10000, -5.4, -3.8, 10000],\n","            'n_big_aux':\t[-0.01, 3.1, 1000],\n","            'vol_big_ratio_aux':\t\t[-0.01, 0.275, 0.520, 1.01],\n","            'loc_agg_net_m_aux':\t\t[-10000, -4.451, -3.592, -2.435, 3.607, 10000] ,\n","            'loc_aggbig_imb_m_aux':\t\t[-1.1, -0.99, -0.391, -0.0118, 0.4, 0.99, 1.1],\n","            'aggpior_DIF_30':\t\t[-10000, -4.454, -4.111, -3.829, -3.26, -3.05, -3.04, -0.01, 0.01, 3.714, 10000],\n","            'abs_DIF':\t\t[-10000, -4.796, -4.19, -0.1, 0.1, 2.398, 4.564, 10000],\n","            'pagg_DIF':\t[-1.1, -0.95, -0.6, -0.4, -0.001, 0.001, 0.4, 0.6, 0.95, 1.1],\n","            'book_imb_dif':\t[-1000, -27, -14, -6.8, -2.5, 0.1, 4.29, 11.2, 22, 1000],\n","            'book_imb_mean_40': [-10000, -30, -20, -16, 10000],\n","            'aggpior_aux_DIF':\t\t[-10000, -4.331, -3.584, -3.045, -0.01, 0.01, 3.045, 10000] ,\n","            'pagg_aux_DIF':\t\t[-1.1, -0.95, -0.5, -0.001, 0.001, 0.5, 0.95, 1.1],\n","            'escora_3.5_1.2_DIF':\t\t[-1.1, -0.171, -0.0482, 0, 0.0482, 0.0927, 1.1],\n","            'movesc_3.5_0.7_DIF':\t\t[-10000, -4.451, -1.391, -0.146, 0.0, 0.312, 10000],\n","            'book_imb_mean_dif_cp_ABS':\t\t[-0.01, 0.119, 10000],\n","            'msg_imb_mean_dif_lp_ABS':\t\t[-0.01, 0.281, 1.1],\n","            'loc_agg_imb_m':  [-1.01, -0.491, -0.366, -0.288, -0.223, -0.167, 1.01],\n","            'imp_FCAST_10': [-10000, -3.012, -1.674, -0.653, 10000],\n","}\n","\n","l_bins_nulls = ['vol_big','n_big','PA_up','PA_down','vol_big_aux','n_aggimb_aux','imp_DIF','int_DIF',\n","                'int_DIF_50','abs_DIF_30','msg_imb_dif','depth_DIF',\n","                'depth_DIF_10','book_imb_mean_us_5','book_imb_mean_us_20','sspread_mean_us_5',\n","                'escora_2.5_2_DIF','movesc_4.5_0.7_DIF', \n","                # TODO: 's_run', 'n_p_aux' treat for MW\n","]\n","\n","# ft classification based on mean encoding \n","l_alta = ['pagg_DIF','loc_agg_net_m_aux','aggpior_aux_DIF','loc_agg_imb_m','vol_big_ratio','aggimb',\n","             'n_aggimb','agg_net_m','loc_aggbig_imb_m','n_p','vewma','agg_net_m_aux','smart_price_dif',\n","             'smart_price_50','agg_net_10','msg_imb_mean_40_ABS','ohlc_10','ohlc_50','loc_aggbig_net_m',\n","             'vol_trd_aux','aggbig_net_m_aux','rng_ewma_dif_80','vewma_10','rng_ewma','vewma_g_p','vewmag_dif',\n","             'n_trd_aux','escora_bid_2.5_0.8','escora_ask_2.5_0.8','escora_bid_3.5_0.8','escora_ask_3.5_0.8'\n","             ,'escora_bid_4.5_1.2','escora_ask_4.5_1.2','movesc_bid_3.5_0.5','movesc_ask_3.5_0.5','msg_imb','rng_smart_10',\n","]\n","\n","l_media = ['last_d_s','aggimb_big','loc_aggbig_imb_m_aux','pagg_aux_DIF','movesc_3.5_0.7_DIF','abs_DIF',\n","              'book_imb_mean_dif_cp_ABS','book_imb_dif','abagg','vewma_c_v','aggimb_aux','loc_aggbig_net_m_aux',\n","              'rng_smart_50','agg_net_40','loc_agg_net_10','int_DIF_10','abagg_10','msg_imb_mean_dif_cp',\n","              'sspread_mean','movesc_2.5_0.7_DIF','loc_agg_net_m','book_imb','rng_ewma_dif','rng_ewma_dif_40',\n","              'vewma_g_p_10','aggbig_net_10','vol_trd','loc_agg_imb_m_aux','movesc_bid_2.5_0.9','movesc_ask_2.5_0.9',\n","              'movesc_bid_4.5_0.9','movesc_ask_4.5_0.9','imp_DIF_10','imp_FCAST_40','aggpior_DIF','book_imb_mean_dif_cp',\n","              'msg_imb_mean_10',\n","]\n","\n","# best features from RecursiveFeatureAdd/RandomForest, threshold= 0.002\n","l_best = ['ohlc_50', 'vewma', 'vewmag_dif', 'rng_smart_10', 'ohlc_10',\n","          'loc_agg_imb_m', 'aggpior_DIF', 'agg_net_10', 'smart_price_dif',\n","          'escora_bid_3.5_0.8', 'msg_imb_mean_40_ABS'\n","]\n","\n","l_ambas = ['movesc_ask_2.5_0.9', 'smart_price_dif', 'ohlc_50', 'ohlc_10',\n","           'movesc_bid_2.5_0.9', 'loc_agg_imb_m', 'loc_agg_net_m_aux', 'rng_smart_10',\n","]\n","\n","l_tm = ['chgfreq','msg_imb_mean_dif_cp','pagg_aux_DIF','msg_imb','imp_FCAST_10','imp_FCAST_40',]\n","\n","l_linear = ['pagg_DIF','vol_trd','vewmag_dif','msg_imb_mean_40_ABS','n_p','rng_ewma_dif',]\n","\n","l_tree=['agg_net_10', 'vewma', 'vewma_c_v', 'agg_net_40', 'imp_DIF_10', 'book_imb_mean_dif_cp',]\n","\n","l_best_svm = ['smart_price_dif', 'ohlc_50', 'chgfreq','movesc_ask_2.5_0.9']\n","\n","l_second_svm = ['vewmag_dif', 'msg_imb', 'vol_trd', 'n_p', 'ohlc_10',\n","                'msg_imb_mean_40_ABS', 'loc_agg_net_m_aux_me', 'loc_agg_imb_m_me',\n","                'pagg_DIF_me', 'pagg_aux_DIF_me']\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["pipe_cap_outliers = Pipeline([\n","                ('cap1', Winsorizer(variables=l_cap_1, capping_method='quantiles', fold = 0.01, tail = 'both')),\n","                ('cap5', Winsorizer(variables=l_cap_5, capping_method='quantiles', fold = 0.05, tail = 'both')),\n","                ('cap10', Winsorizer(variables=l_cap_10, capping_method='quantiles', fold = 0.10, tail = 'both')),\n","                ('cap001', Winsorizer(variables=l_cap_default, capping_method='quantiles', fold = 0.001, tail = 'both')),\n","])\n","\n","pipe_norm_scale = Pipeline([\n","                ('scaler', SklearnTransformerWrapper(transformer=RobustScaler(quantile_range=(0.10, 0.90)))),\n","                ('minmax', SklearnTransformerWrapper(transformer=MinMaxScaler())),\n","                # ('pca', PCA(n_components=30, svd_solver='auto')), \n","                # ('isomap', Isomap(n_components=13, n_neighbors=50, n_jobs=-2)),   #  expensive\n","])\n","\n","pipe_discrete = Pipeline([('drop', DropFeatures(l_bins_nulls)),\n","                        ('duplicate', ft_eng.Duplicate(l_duplicate)),\n","                        ('outliers_all', Winsorizer(capping_method='quantiles', fold = 0.001, tail = 'both')),\n","                        ('bins_manual', ArbitraryDiscretiser(d_bins_arbitrary)),\n","                        ('bins_q10', EqualFrequencyDiscretiser(return_object=False, q=10, variables=l_bins_q10)),\n","                        ('bins_q8', EqualFrequencyDiscretiser(return_object=False, q=8, variables=l_bins_q8)),\n","                        ('bins_q6', EqualFrequencyDiscretiser(return_object=False, q=6, variables=l_bins_q6)),\n","                        ('bins_q5', EqualFrequencyDiscretiser(return_object=False, q=5, variables=l_bins_q5)),\n","                        ('bins_q4', EqualFrequencyDiscretiser(return_object=False, q=4, variables=l_bins_q4)),\n","                        ('bins_q2', EqualFrequencyDiscretiser(return_object=False, q=2, variables=l_bins_q2)),\n","])\n","\n","pipe_mean_encoding = Pipeline([('drop', DropFeatures(l_bins_nulls)),\n","                        ('duplicate', ft_eng.Duplicate(l_duplicate)),\n","                        ('outliers_all', Winsorizer(capping_method='quantiles', fold = 0.001, tail = 'both')),\n","                        ('bins_manual', ArbitraryDiscretiser(d_bins_arbitrary)),\n","                        ('bins_q10', EqualFrequencyDiscretiser(return_object=True, q=10, variables=l_bins_q10)),\n","                        ('bins_q8', EqualFrequencyDiscretiser(return_object=True, q=8, variables=l_bins_q8)),\n","                        ('bins_q6', EqualFrequencyDiscretiser(return_object=True, q=6, variables=l_bins_q6)),\n","                        ('bins_q5', EqualFrequencyDiscretiser(return_object=True, q=5, variables=l_bins_q5)),\n","                        ('bins_q4', EqualFrequencyDiscretiser(return_object=True, q=4, variables=l_bins_q4)),\n","                        ('bins_q2', EqualFrequencyDiscretiser(return_object=True, q=2, variables=l_bins_q2)),\n","                        ('mean_enc', MeanEncoder())\n","])\n","\n","# X_train_transf = pipe_mean_encoding.fit_transform(X_train, y_train)\n","# X_test_pre = pipe_cap_outliers.transform(X_test)\n","\n","# DEBUG: must return all zeroes \n","# pd.isna(X_train_transf).sum().sort_values(ascending=False)\n","\n",""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import time \n","def epoch2str(epoch):\n","  # from Fredy\n","  mlsec = \"000\"\n","  if str(epoch).find(\".\") >= 0:\n","        mlsec = repr(epoch).split('.')[1][:3]\n","  return time.strftime(\n","      '[%Y-%m-%d %H:%M:%S.{}]'.format(mlsec), time.localtime(epoch))\n","\n",""]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["\n","pipe_s = Pipeline([('cap_outliers', pipe_cap_outliers),\n","                  ('norm_scale', pipe_norm_scale)\n","                  ])\n","pipe_s.fit(X_train, y_train) \n","X_scale = pipe_s.transform(X_train)\n","X_test_scale = pipe_s.transform(X_test)\n","\n","pipe_mean_encoding.fit(X_train, y_train) \n","X_me = pipe_mean_encoding.transform(X_train)\n","X_test_me = pipe_mean_encoding.transform(X_test)\n","\n","pipe_discrete.fit(X_train, y_train) \n","X_d = pipe_discrete.transform(X_train)\n","X_test_d = pipe_discrete.transform(X_test)\n","\n","X_ens_opt = X_scale.join(X_me, how='outer', rsuffix='_me', sort=False).join(X_d, how='outer', rsuffix='_d', sort=False)\n","X_ens_opt_test = X_test_scale.join(X_test_me, how='outer', rsuffix='_me', sort=False).join(X_test_d, how='outer', rsuffix='_d', sort=False)\n","\n","# feature selection for ensemble models\n","d_run_models = {'xgb1': 'scale_discrete',    # based on this, specify _d or _me for discrete or mean encoding\n","                'svm1': 'scale_me',           # on d_ft_bins below\n","                'knn57': 'scale',\n","                'knn75': 'scale',\n","                'knn100': 'scale',\n","                'logit1': 'me',\n","                'logit2': 'me'\n","\n","}\n","\n","d_ft_bins = {'xgb1': ['smart_price_dif_d','chgfreq_d','agg_net_10_d','book_imb_mean_dif_cp_ABS_d',],\n","             'svm1': ['ohlc_50', 'loc_agg_imb_m', 'chgfreq', ],\n","             'knn57': 'NA',\n","             'knn75': 'NA',\n","             'knn100': 'NA',\n","             'logit1': ['agg_net_m', 'n_p', 'chgfreq', 'vewma_c_v', 'aggimb_big_aux', 'smart_price_dif', \n","                        'smart_price_50', 'agg_net_10', 'int_DIF_10', 'msg_imb_mean_40_ABS'],\n","             'logit2': ['agg_net_m', 'chgfreq', 'smart_price_dif', 'smart_price_50', 'agg_net_10']\n","}\n","\n","d_ft_scale = {'xgb1': ['smart_price_dif','ohlc_10','chgfreq','agg_net_10',],\n","              'svm1': ['imp_FCAST_10', 'rng_smart_10', 'smart_price_dif', 'movesc_ask_2.5_0.9', 'msg_imb_mean_dif_cp', 'pagg_aux_DIF'],\n","              'knn57': ['chgfreq', 'smart_price_dif', 'rng_smart_10', ],  # 'ohlc_50', 'loc_agg_net_m_aux' \n","              'knn75': ['book_imb_mean_dif_cp', 'imp_FCAST_10', 'imp_FCAST_40', 'pagg_aux_DIF', 'loc_agg_net_m_aux', 'n_trd_aux', 'msg_imb',],\n","              'knn100': ['n_trd',  'vol_big',  'aggimb_big',  'loc_agg_imb_m',  'chgfreq',  'PA_down',  \n","                        'vewma',  'movesc_ask_4.5_0.9',  'smart_price_dif',  'ohlc_10',  'ohlc_50',  \n","                        'vewma_g_p_10', 'int_DIF_50', 'book_imb_dif', 'book_imb_mean_dif_cp', 'msg_imb_mean_40_ABS'],\n","              'logit1': 'NA',\n","              'logit2': 'NA'\n","}\n","\n","\n","from sklearn.metrics import fbeta_score, make_scorer\n","f_beta_scorer = make_scorer(fbeta_score, beta=0.5)\n",""]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["y   2.312\n","dtype: float64"]},"metadata":{},"execution_count":13}],"source":"y_train.count()/y_train.sum()"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------\n","ENSEMBLE: 8\n","Initial time: [2021-06-10 20:10:35.192]\n","\n","Ensemble results for fixed parameters: \n","train_auc: 0.6403, test_auc: 0.6112 (overfit: 0.0291)\n","train_f1:  0.5775, test_f1:  0.5501 (overfit: 0.0274)\n","train_f*:  0.5365, test_f*:  0.5187 (overfit: 0.0178)\n","--------\n","cm_train: \n","[[3869 3162]\n"," [1899 3459]]\n","cm_test: \n","[[1013  767]\n"," [ 527  791]]\n","threshold: 0.470\n","\n","Final time: [2021-06-10 20:14:09.577]\n"]}],"source":["N_ENS = 8\n","b_optimize = False\n","\n","print('-------------------------------------------')\n","print('ENSEMBLE: {}'.format(N_ENS))\n","print('Initial time: {}'.format(epoch2str(time.time())))\n","\n","l_estimators = [\n","              ('svm1',Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['svm1']+d_ft_bins['svm1']))),\n","                                ('clf', SVC(probability=True, \n","                                          class_weight='balanced', cache_size=1000, \n","                                          C=24.84969151880429, \n","                                          gamma= 0.0030157367155080337))])\n","              ), \n","              ('xgb1', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['xgb1']+d_ft_bins['xgb1']))),\n","                                ('clf', XGBClassifier( objective='binary:logistic'\n","                                                      ,eval_metric='auc'\n","                                                      ,use_label_encoder=False\n","                                                      ,tree_method='auto'\n","                                                      ,max_depth=5\n","                                                      ,subsample=0.7\n","                                                      ,colsample_bytree=0.99999\n","                                                      ,min_child_weight=25\n","                                                      ,learning_rate=0.0015\n","                                                      ,n_estimators=1100\n","                                                      ,gamma=3\n","                                                      ,reg_alpha=2\n","                                                      ,scale_pos_weight=2.5))])\n","              ), \n","              ('knn57', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn57']))),\n","                                  ('clf', KNeighborsClassifier(n_neighbors=57))])\n","              ), \n","              # ('knn75', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn75']))),\n","              #                     ('clf', KNeighborsClassifier(n_neighbors=75))])\n","              # ), \n","              # ('knn100', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn100']))),\n","              #                      ('clf', KNeighborsClassifier(n_neighbors=100))])\n","              # ), \n","              # ('logit1', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_bins['logit1']))),\n","              #                      ('clf', LogisticRegression(max_iter=800))])\n","              # ), \n","              # ('logit2', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_bins['logit2']))),\n","              #                      ('clf', LogisticRegression(max_iter=800))])\n","              # ), \n","]\n","\n","\n","# search params OR run fix models\n","if b_optimize:\n","  search_space = list()\n","  param_name = []\n","\n","  ### final estimator\n","  param_name.append('logit_C') \n","  search_space.append(Real(1e-6, 100.0, 'log-uniform', name='logit_C'))\n","\n","  # define the function used to evaluate a given configuration\n","  @use_named_args(search_space)\n","  def evaluate_model(**params):\n","\n","    ens_opt = StackingClassifier(estimators=l_estimators, \n","                                final_estimator=LogisticRegression(dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, \n","                                                                  random_state=None, max_iter=100, multi_class='auto',\n","                                                                  verbose=0, warm_start=False, n_jobs=-2, l1_ratio=None,\n","                                                                  penalty='l1', solver='liblinear', class_weight='balanced',\n","                                                                  C=params['logit_C']),\n","                                # final_estimator=MajorityVote(),\n","                                # final_estimator=KNeighborsClassifier(),\n","                                passthrough=False, cv=3, verbose=1, n_jobs=-2\n","                                )\n","\n","    cv_results = cross_validate(ens_opt, X_ens_opt, y_train, cv=4, n_jobs=-2, scoring=f_beta_scorer, return_train_score= True)\n","    \n","    overfit_devs = abs(cv_results['train_score'].mean() - cv_results['test_score'].mean())\n","    estimate = cv_results['test_score'].mean() - cv_results['test_score'].std() / 4 - overfit_devs / 3\n","\n","    return 1.0 - estimate               \n","\n","\n","  result_ens_opt = forest_minimize(evaluate_model, search_space, n_calls=100, n_jobs=-2, verbose=1, \n","                                        kappa=4.00, callback=DeltaYStopper(0.0002, n_best=8))\n","      \n","  # summarizing finding:\n","  print('\\noptimized_ensemble():')\n","  print('Best Metric: %.3f' % (1.0 - result_ens_opt.fun))\n","  for i, val in enumerate(result_ens_opt.x):\n","    print('{}: {}'.format(param_name[i],val))\n","\n","  from skopt.plots import plot_evaluations\n","  _ = plot_evaluations(result_ens_opt, bins=10)\n","\n","  from skopt.plots import plot_objective\n","  _ = plot_objective(result_ens_opt)\n","\n","  print('Time at end: {}'.format(epoch2str(time.time())))\n","\n","else:\n","  ens_opt = StackingClassifier(estimators=l_estimators, \n","                              # final_estimator=LogisticRegression(dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, \n","                              #                                   random_state=None, max_iter=100, multi_class='auto',\n","                              #                                   verbose=0, warm_start=False, n_jobs=-2, l1_ratio=None,\n","                              #                                   penalty='l1', solver='liblinear', class_weight='balanced',),\n","                              final_estimator=RandomForestClassifier(max_depth=3, n_jobs=-2, max_samples=0.6),\n","                              passthrough=False, cv=3, verbose=1, n_jobs=-2\n","                              )\n","\n","# fit ensemble model and predict \n","# ens_opt.set_params(final_estimator__C=0.018202370448969933)\n","ens_opt.fit(X_ens_opt, np.ravel(y_train))\n","\n","print('\\nEnsemble results for fixed parameters: ')\n","l_results = tm_train.report_results(X_ens_opt, X_ens_opt_test, y_train, y_test, fitted_model=ens_opt, ready_probs=False, th=0)\n","\n","print('\\nFinal time: {}'.format(epoch2str(time.time())))\n","\n",""]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------\n","ENSEMBLE: 8\n","Initial time: [2021-06-10 20:20:15.438]\n","\n","Ensemble results for fixed parameters: \n","train_auc: 0.6378, test_auc: 0.6076 (overfit: 0.0302)\n","train_f1:  0.5494, test_f1:  0.5238 (overfit: 0.0256)\n","train_f*:  0.5345, test_f*:  0.5197 (overfit: 0.0148)\n","--------\n","cm_train: \n","[[4305 2726]\n"," [2296 3062]]\n","cm_test: \n","[[1131  649]\n"," [ 620  698]]\n","threshold: 0.490\n","\n","Final time: [2021-06-10 20:23:55.524]\n"]}],"source":["N_ENS = 8\n","b_optimize = False\n","\n","print('-------------------------------------------')\n","print('ENSEMBLE: {}'.format(N_ENS))\n","print('Initial time: {}'.format(epoch2str(time.time())))\n","\n","l_estimators = [\n","              ('svm1',Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['svm1']+d_ft_bins['svm1']))),\n","                                ('clf', SVC(probability=True, \n","                                          class_weight='balanced', cache_size=1000, \n","                                          C=24.84969151880429, \n","                                          gamma= 0.0030157367155080337))])\n","              ), \n","              ('xgb1', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['xgb1']+d_ft_bins['xgb1']))),\n","                                ('clf', XGBClassifier( objective='binary:logistic'\n","                                                      ,eval_metric='auc'\n","                                                      ,use_label_encoder=False\n","                                                      ,tree_method='auto'\n","                                                      ,max_depth=5\n","                                                      ,subsample=0.7\n","                                                      ,colsample_bytree=0.99999\n","                                                      ,min_child_weight=25\n","                                                      ,learning_rate=0.0015\n","                                                      ,n_estimators=1100\n","                                                      ,gamma=3\n","                                                      ,reg_alpha=2\n","                                                      ,scale_pos_weight=2.5))])\n","              ), \n","              ('knn57', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn57']))),\n","                                  ('clf', KNeighborsClassifier(n_neighbors=57))])\n","              ), \n","              ('knn75', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn75']))),\n","                                  ('clf', KNeighborsClassifier(n_neighbors=75))])\n","              ), \n","              # ('knn100', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn100']))),\n","              #                      ('clf', KNeighborsClassifier(n_neighbors=100))])\n","              # ), \n","              # ('logit1', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_bins['logit1']))),\n","              #                      ('clf', LogisticRegression(max_iter=800))])\n","              # ), \n","              # ('logit2', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_bins['logit2']))),\n","              #                      ('clf', LogisticRegression(max_iter=800))])\n","              # ), \n","]\n","\n","\n","# search params OR run fix models\n","if b_optimize:\n","  search_space = list()\n","  param_name = []\n","\n","  ### final estimator\n","  param_name.append('logit_C') \n","  search_space.append(Real(1e-6, 100.0, 'log-uniform', name='logit_C'))\n","\n","  # define the function used to evaluate a given configuration\n","  @use_named_args(search_space)\n","  def evaluate_model(**params):\n","\n","    ens_opt = StackingClassifier(estimators=l_estimators, \n","                                final_estimator=LogisticRegression(dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, \n","                                                                  random_state=None, max_iter=100, multi_class='auto',\n","                                                                  verbose=0, warm_start=False, n_jobs=-2, l1_ratio=None,\n","                                                                  penalty='l1', solver='liblinear', class_weight='balanced',\n","                                                                  C=params['logit_C']),\n","                                # final_estimator=MajorityVote(),\n","                                # final_estimator=KNeighborsClassifier(),\n","                                passthrough=False, cv=3, verbose=1, n_jobs=-2\n","                                )\n","\n","    cv_results = cross_validate(ens_opt, X_ens_opt, y_train, cv=4, n_jobs=-2, scoring=f_beta_scorer, return_train_score= True)\n","    \n","    overfit_devs = abs(cv_results['train_score'].mean() - cv_results['test_score'].mean())\n","    estimate = cv_results['test_score'].mean() - cv_results['test_score'].std() / 4 - overfit_devs / 3\n","\n","    return 1.0 - estimate               \n","\n","\n","  result_ens_opt = forest_minimize(evaluate_model, search_space, n_calls=100, n_jobs=-2, verbose=1, \n","                                        kappa=4.00, callback=DeltaYStopper(0.0002, n_best=8))\n","      \n","  # summarizing finding:\n","  print('\\noptimized_ensemble():')\n","  print('Best Metric: %.3f' % (1.0 - result_ens_opt.fun))\n","  for i, val in enumerate(result_ens_opt.x):\n","    print('{}: {}'.format(param_name[i],val))\n","\n","  from skopt.plots import plot_evaluations\n","  _ = plot_evaluations(result_ens_opt, bins=10)\n","\n","  from skopt.plots import plot_objective\n","  _ = plot_objective(result_ens_opt)\n","\n","  print('Time at end: {}'.format(epoch2str(time.time())))\n","\n","else:\n","  ens_opt = StackingClassifier(estimators=l_estimators, \n","                              # final_estimator=LogisticRegression(dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, \n","                              #                                   random_state=None, max_iter=100, multi_class='auto',\n","                              #                                   verbose=0, warm_start=False, n_jobs=-2, l1_ratio=None,\n","                              #                                   penalty='l1', solver='liblinear', class_weight='balanced',),\n","                              final_estimator=RandomForestClassifier(max_depth=3, n_jobs=-2, max_samples=0.6),\n","                              passthrough=False, cv=3, verbose=1, n_jobs=-2\n","                              )\n","\n","# fit ensemble model and predict \n","# ens_opt.set_params(final_estimator__C=0.018202370448969933)\n","ens_opt.fit(X_ens_opt, np.ravel(y_train))\n","\n","print('\\nEnsemble results for fixed parameters: ')\n","l_results = tm_train.report_results(X_ens_opt, X_ens_opt_test, y_train, y_test, fitted_model=ens_opt, ready_probs=False, th=0)\n","\n","print('\\nFinal time: {}'.format(epoch2str(time.time())))\n","\n",""]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------\n","ENSEMBLE: 9\n","Initial time: [2021-06-10 20:31:51.049]\n","\n","Ensemble results for fixed parameters: \n","train_auc: 0.6450, test_auc: 0.6054 (overfit: 0.0397)\n","train_f1:  0.6022, test_f1:  0.5562 (overfit: 0.0460)\n","train_f*:  0.5398, test_f*:  0.5072 (overfit: 0.0326)\n","--------\n","cm_train: \n","[[3476 3555]\n"," [1518 3840]]\n","cm_test: \n","[[902 878]\n"," [472 846]]\n","threshold: 0.470\n","\n","Final time: [2021-06-10 20:35:39.809]\n"]}],"source":["N_ENS = 9\n","b_optimize = False\n","\n","print('-------------------------------------------')\n","print('ENSEMBLE: {}'.format(N_ENS))\n","print('Initial time: {}'.format(epoch2str(time.time())))\n","\n","l_estimators = [\n","              ('svm1',Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['svm1']+d_ft_bins['svm1']))),\n","                                ('clf', SVC(probability=True, \n","                                          class_weight='balanced', cache_size=1000, \n","                                          C=24.84969151880429, \n","                                          gamma= 0.0030157367155080337))])\n","              ), \n","              ('xgb1', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['xgb1']+d_ft_bins['xgb1']))),\n","                                ('clf', XGBClassifier( objective='binary:logistic'\n","                                                      ,eval_metric='auc'\n","                                                      ,use_label_encoder=False\n","                                                      ,tree_method='auto'\n","                                                      ,max_depth=5\n","                                                      ,subsample=0.7\n","                                                      ,colsample_bytree=0.99999\n","                                                      ,min_child_weight=25\n","                                                      ,learning_rate=0.0015\n","                                                      ,n_estimators=1100\n","                                                      ,gamma=3\n","                                                      ,reg_alpha=2\n","                                                      ,scale_pos_weight=2.5))])\n","              ), \n","              ('knn57', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn57']))),\n","                                  ('clf', KNeighborsClassifier(n_neighbors=57))])\n","              ), \n","              ('knn75', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn75']))),\n","                                  ('clf', KNeighborsClassifier(n_neighbors=75))])\n","              ), \n","              ('knn100', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn100']))),\n","                                   ('clf', KNeighborsClassifier(n_neighbors=100))])\n","              ), \n","              # ('logit1', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_bins['logit1']))),\n","              #                      ('clf', LogisticRegression(max_iter=800))])\n","              # ), \n","              # ('logit2', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_bins['logit2']))),\n","              #                      ('clf', LogisticRegression(max_iter=800))])\n","              # ), \n","]\n","\n","\n","# search params OR run fix models\n","if b_optimize:\n","  search_space = list()\n","  param_name = []\n","\n","  ### final estimator\n","  param_name.append('logit_C') \n","  search_space.append(Real(1e-6, 100.0, 'log-uniform', name='logit_C'))\n","\n","  # define the function used to evaluate a given configuration\n","  @use_named_args(search_space)\n","  def evaluate_model(**params):\n","\n","    ens_opt = StackingClassifier(estimators=l_estimators, \n","                                final_estimator=LogisticRegression(dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, \n","                                                                  random_state=None, max_iter=100, multi_class='auto',\n","                                                                  verbose=0, warm_start=False, n_jobs=-2, l1_ratio=None,\n","                                                                  penalty='l1', solver='liblinear', class_weight='balanced',\n","                                                                  C=params['logit_C']),\n","                                # final_estimator=MajorityVote(),\n","                                # final_estimator=KNeighborsClassifier(),\n","                                passthrough=False, cv=3, verbose=1, n_jobs=-2\n","                                )\n","\n","    cv_results = cross_validate(ens_opt, X_ens_opt, y_train, cv=4, n_jobs=-2, scoring=f_beta_scorer, return_train_score= True)\n","    \n","    overfit_devs = abs(cv_results['train_score'].mean() - cv_results['test_score'].mean())\n","    estimate = cv_results['test_score'].mean() - cv_results['test_score'].std() / 4 - overfit_devs / 3\n","\n","    return 1.0 - estimate               \n","\n","\n","  result_ens_opt = forest_minimize(evaluate_model, search_space, n_calls=100, n_jobs=-2, verbose=1, \n","                                        kappa=4.00, callback=DeltaYStopper(0.0002, n_best=8))\n","      \n","  # summarizing finding:\n","  print('\\noptimized_ensemble():')\n","  print('Best Metric: %.3f' % (1.0 - result_ens_opt.fun))\n","  for i, val in enumerate(result_ens_opt.x):\n","    print('{}: {}'.format(param_name[i],val))\n","\n","  from skopt.plots import plot_evaluations\n","  _ = plot_evaluations(result_ens_opt, bins=10)\n","\n","  from skopt.plots import plot_objective\n","  _ = plot_objective(result_ens_opt)\n","\n","  print('Time at end: {}'.format(epoch2str(time.time())))\n","\n","else:\n","  ens_opt = StackingClassifier(estimators=l_estimators, \n","                              # final_estimator=LogisticRegression(dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, \n","                              #                                   random_state=None, max_iter=100, multi_class='auto',\n","                              #                                   verbose=0, warm_start=False, n_jobs=-2, l1_ratio=None,\n","                              #                                   penalty='l1', solver='liblinear', class_weight='balanced',),\n","                              final_estimator=RandomForestClassifier(max_depth=3, n_jobs=-2, max_samples=0.6),\n","                              passthrough=False, cv=3, verbose=1, n_jobs=-2\n","                              )\n","\n","# fit ensemble model and predict \n","# ens_opt.set_params(final_estimator__C=0.018202370448969933)\n","ens_opt.fit(X_ens_opt, np.ravel(y_train))\n","\n","print('\\nEnsemble results for fixed parameters: ')\n","l_results = tm_train.report_results(X_ens_opt, X_ens_opt_test, y_train, y_test, fitted_model=ens_opt, ready_probs=False, th=0)\n","\n","print('\\nFinal time: {}'.format(epoch2str(time.time())))\n","\n",""]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------\n","ENSEMBLE: 11\n","Initial time: [2021-06-10 20:43:02.868]\n","\n","Ensemble results for fixed parameters: \n","train_auc: 0.6331, test_auc: 0.6044 (overfit: 0.0287)\n","train_f1:  0.5745, test_f1:  0.5459 (overfit: 0.0286)\n","train_f*:  0.5337, test_f*:  0.5186 (overfit: 0.0151)\n","--------\n","cm_train: \n","[[3849 3182]\n"," [1916 3442]]\n","cm_test: \n","[[1031  749]\n"," [ 542  776]]\n","threshold: 0.470\n","\n","Final time: [2021-06-10 20:46:42.800]\n"]}],"source":["N_ENS = 11\n","b_optimize = False\n","\n","print('-------------------------------------------')\n","print('ENSEMBLE: {}'.format(N_ENS))\n","print('Initial time: {}'.format(epoch2str(time.time())))\n","\n","l_estimators = [\n","              ('svm1',Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['svm1']+d_ft_bins['svm1']))),\n","                                ('clf', SVC(probability=True, \n","                                          class_weight='balanced', cache_size=1000, \n","                                          C=24.84969151880429, \n","                                          gamma= 0.0030157367155080337))])\n","              ), \n","              ('xgb1', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['xgb1']+d_ft_bins['xgb1']))),\n","                                ('clf', XGBClassifier( objective='binary:logistic'\n","                                                      ,eval_metric='auc'\n","                                                      ,use_label_encoder=False\n","                                                      ,tree_method='auto'\n","                                                      ,max_depth=5\n","                                                      ,subsample=0.7\n","                                                      ,colsample_bytree=0.99999\n","                                                      ,min_child_weight=25\n","                                                      ,learning_rate=0.0015\n","                                                      ,n_estimators=1100\n","                                                      ,gamma=3\n","                                                      ,reg_alpha=2\n","                                                      ,scale_pos_weight=2.5))])\n","              ), \n","              ('knn57', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn57']))),\n","                                  ('clf', KNeighborsClassifier(n_neighbors=57))])\n","              ), \n","              ('knn75', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn75']))),\n","                                  ('clf', KNeighborsClassifier(n_neighbors=75))])\n","              ), \n","              # ('knn100', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_scale['knn100']))),\n","              #                      ('clf', KNeighborsClassifier(n_neighbors=100))])\n","              # ), \n","              ('logit1', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_bins['logit1']))),\n","                                   ('clf', LogisticRegression(max_iter=800))])\n","              ), \n","              # ('logit2', Pipeline([('ft_sel', DropFeatures(list_sub(X_ens_opt.columns.to_list(),d_ft_bins['logit2']))),\n","              #                      ('clf', LogisticRegression(max_iter=800))])\n","              # ), \n","]\n","\n","\n","# search params OR run fix models\n","if b_optimize:\n","  search_space = list()\n","  param_name = []\n","\n","  ### final estimator\n","  param_name.append('logit_C') \n","  search_space.append(Real(1e-6, 100.0, 'log-uniform', name='logit_C'))\n","\n","  # define the function used to evaluate a given configuration\n","  @use_named_args(search_space)\n","  def evaluate_model(**params):\n","\n","    ens_opt = StackingClassifier(estimators=l_estimators, \n","                                final_estimator=LogisticRegression(dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, \n","                                                                  random_state=None, max_iter=100, multi_class='auto',\n","                                                                  verbose=0, warm_start=False, n_jobs=-2, l1_ratio=None,\n","                                                                  penalty='l1', solver='liblinear', class_weight='balanced',\n","                                                                  C=params['logit_C']),\n","                                # final_estimator=MajorityVote(),\n","                                # final_estimator=KNeighborsClassifier(),\n","                                passthrough=False, cv=3, verbose=1, n_jobs=-2\n","                                )\n","\n","    cv_results = cross_validate(ens_opt, X_ens_opt, y_train, cv=4, n_jobs=-2, scoring=f_beta_scorer, return_train_score= True)\n","    \n","    overfit_devs = abs(cv_results['train_score'].mean() - cv_results['test_score'].mean())\n","    estimate = cv_results['test_score'].mean() - cv_results['test_score'].std() / 4 - overfit_devs / 3\n","\n","    return 1.0 - estimate               \n","\n","\n","  result_ens_opt = forest_minimize(evaluate_model, search_space, n_calls=100, n_jobs=-2, verbose=1, \n","                                        kappa=4.00, callback=DeltaYStopper(0.0002, n_best=8))\n","      \n","  # summarizing finding:\n","  print('\\noptimized_ensemble():')\n","  print('Best Metric: %.3f' % (1.0 - result_ens_opt.fun))\n","  for i, val in enumerate(result_ens_opt.x):\n","    print('{}: {}'.format(param_name[i],val))\n","\n","  from skopt.plots import plot_evaluations\n","  _ = plot_evaluations(result_ens_opt, bins=10)\n","\n","  from skopt.plots import plot_objective\n","  _ = plot_objective(result_ens_opt)\n","\n","  print('Time at end: {}'.format(epoch2str(time.time())))\n","\n","else:\n","  ens_opt = StackingClassifier(estimators=l_estimators, \n","                              # final_estimator=LogisticRegression(dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, \n","                              #                                   random_state=None, max_iter=100, multi_class='auto',\n","                              #                                   verbose=0, warm_start=False, n_jobs=-2, l1_ratio=None,\n","                              #                                   penalty='l1', solver='liblinear', class_weight='balanced',),\n","                              final_estimator=RandomForestClassifier(max_depth=3, n_jobs=-2, max_samples=0.6),\n","                              passthrough=False, cv=3, verbose=1, n_jobs=-2\n","                              )\n","\n","# fit ensemble model and predict \n","# ens_opt.set_params(final_estimator__C=0.018202370448969933)\n","ens_opt.fit(X_ens_opt, np.ravel(y_train))\n","\n","print('\\nEnsemble results for fixed parameters: ')\n","l_results = tm_train.report_results(X_ens_opt, X_ens_opt_test, y_train, y_test, fitted_model=ens_opt, ready_probs=False, th=0)\n","\n","print('\\nFinal time: {}'.format(epoch2str(time.time())))\n","\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}